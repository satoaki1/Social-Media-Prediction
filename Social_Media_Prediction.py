# -*- coding: utf-8 -*-
"""ITS66604_MidTerm_0354208.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZQNioET30zqvT9o1UjV94epI6HHXDUoE

## Import basic libraries for EDA and create DataFrame
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# load data into dataframe object named 'df'
df = pd.read_csv("KAG_conversion_data.csv")

"""# Source Codes for Question 1 - EDA demonstration

### Dataset Information gathering
"""

# Column names, Non-Null value numbers of each column, and data type of the dataset
df.info()

# Overall information such as mean, min, max, etc. of the dataset (numerical data only)
df.describe()

# Numbers of rows and columns in the dataset
df.shape

df.isnull().sum()

# data types of each column in the dataset
df.dtypes

# Categorical data exploration with describe() function
categorical_columns = df.select_dtypes(include = "object")
categorical_columns.describe()

# Check what kind of values are existing in each frequency, for each categorical column
for column in categorical_columns:
    frequency_table = df[column].value_counts()
    print(frequency_table)

"""### Data Visualization"""

# 1. Distribution of Numerical Variables
# Histograms for each numeric feature
numeric_features = df.select_dtypes(include=[np.number]).columns
for feature in numeric_features:
    plt.figure(figsize=(10,6))
    sns.histplot(df[feature], kde=True, bins=30)
    plt.title(f'Distribution of {feature}')
    plt.show()

# 2. Relationship between Numerical Features and Total Conversion
# Scatter plots for numeric features vs Total Conversion
for feature in numeric_features:
    if feature != 'Total_Conversion':
        plt.figure(figsize=(10,6))
        sns.scatterplot(data=df, x=feature, y='Total_Conversion')
        plt.title(f'{feature} vs Total Conversion')
        plt.show()

# 3. Distribution of Categorical Variables
# Bar plots for categorical features (age and gender)
categorical_features = ['age', 'gender']
for feature in categorical_features:
    plt.figure(figsize=(10,6))
    sns.countplot(data=df, x=feature, order=df[feature].value_counts().index)
    plt.title(f'Distribution of {feature}')
    plt.show()

# 4. Correlation Matrix
plt.figure(figsize=(12,8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# 5. Seaborn Pairplot
plt.figure(figsize=(12,8))
sns.pairplot(df)
plt.show()

"""### Categorical Data Encoding"""

# Import essential libraries and classes for encoding
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

"""**Label Encoding**"""

# Creating an instance of LabelEncoder class
label_encoder = LabelEncoder()

# Creating a deep copy of the dataframe 'df' to avoid modifying the original dataframe
df_le = df.copy()

# Using the LabelEncoder to transform the 'age' column of the dataframe
# This will convert distinct age values into distinct integer labels
df_le["age"] = label_encoder.fit_transform(df["age"])

# Using the LabelEncoder again to transform the 'gender' column of the dataframe
# This will convert distinct gender values into distinct integer labels
df_le["gender"] = label_encoder.fit_transform(df["gender"])

df_le

"""**One-Hot Encoding**"""

# Creating an instance of OneHotEncoder class
one_hot_encoder = OneHotEncoder(sparse=False)

# Creating a deep copy of the dataframe 'df' to avoid modifying the original dataframe
df_ohe = df.copy()

# Using the OneHotEncoder to transform the 'age' column of the dataframe
# This converts each distinct age value into a binary (0 or 1) representation across multiple columns
# The result is a matrix where each column corresponds to a unique age value and rows have a 1 if that age is present, 0 otherwise
df_ohe_encoded = one_hot_encoder.fit_transform(df[['age']])

# Appending the one-hot encoded 'age' columns to the 'df_ohe' dataframe
# The column names for the one-hot encoded data are derived from the categories detected by the encoder
df_ohe = pd.concat([df_ohe, pd.DataFrame(df_ohe_encoded, columns = one_hot_encoder.categories_[0])], axis = 1)

df_ohe

# Using the OneHotEncoder to transform the 'gender' column of the dataframe
# This converts each distinct gender value into a binary (0 or 1) representation across multiple columns
# The result is a matrix where each column corresponds to a unique gender value and rows have a 1 if that gender is present, 0 otherwise
df_ohe_encoded = one_hot_encoder.fit_transform(df[['gender']])

# Appending the one-hot encoded 'gender' columns to the 'df_ohe' dataframe
# The column names for the one-hot encoded data are derived from the categories detected by the encoder
df_ohe = pd.concat([df_ohe, pd.DataFrame(df_ohe_encoded, columns = one_hot_encoder.categories_[0])], axis = 1)

df_ohe

# Since encoding for "age" and "gender" has been done, drop the two categorical columns from the df_ohe object
df_ohe = df_ohe.drop(["age", "gender"], axis = 1)

df_ohe

# Correlation Matrix with the encoded dataset
plt.figure(figsize=(12,8))
sns.heatmap(df_ohe.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""# Source codes for Question 2 - Build a Linear Regression model and evaluate"""

# Import libraries essential in Question 2
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Splitting the data

# Features for training
X = df_ohe.drop(["interest", "Total_Conversion",
                 "30-34", "35-39", "40-44", "45-49", "F", "M"], axis=1)

# Target Variable
y = df['Total_Conversion']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

# Training the Baseline Linear Regression Model
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Calculate Coefficient
coef_df = pd.DataFrame({"Feature": X.columns, "Coefficient": model.coef_})
print(coef_df)

# Calculate Intercept
print("Intercept: ", round(model.intercept_, 6))

# Evaluate the model with evaluation metrics
baseline_mse = mean_squared_error(y_test, y_pred)
baseline_rmse = round(np.sqrt(baseline_mse), 6)

# Print the output
print(f"Root Mean Squared Error: {baseline_rmse}")

"""### Source Codes for Question 3 - Polynomial Regression model demonstration"""

# Import libraries essential in Question 3
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
import numpy as np

# 1. Train polynomial regression models with different degrees.
degrees = [1, 2, 3, 4, 5, 6]
models = {}
for degree in degrees:
    pipeline = make_pipeline(PolynomialFeatures(degree, include_bias=False), LinearRegression())
    pipeline.fit(X_train, y_train)
    models[degree] = pipeline

# 2. Evaluate and find the optimal degree for the polynomial regression model.
best_degree = None
best_rmse = float('inf')
for degree, model in models.items():
    predictions = model.predict(X_test)
    mse = mean_squared_error(y_test, predictions)
    rmse = round(np.sqrt(mse), 6)

    print(f"Degree {degree} RMSE: {rmse}")

    if rmse < best_rmse:
        best_rmse = rmse
        best_degree = degree

print(f"\nOptimal Degree: {best_degree}")

# 3. Compare the features in the polynomial regression model to the baseline model.
poly_features = PolynomialFeatures(degree=best_degree, include_bias=False)
poly_features.fit(X_train)
feature_names = poly_features.get_feature_names_out(input_features=X_train.columns)

print(f"Original Features: {X_train.columns.tolist() if hasattr(X_train, 'columns') else 'Column names not available'}")
print(f"\nPolynomial Features (Degree {best_degree}):")
for name in feature_names:
    print(name)

# 4. Get the coefficients and intercept for the best model
best_model = models[best_degree]
linear_reg_model = best_model.named_steps['linearregression']

coefficients = linear_reg_model.coef_
intercept = round(linear_reg_model.intercept_, 6)

print("Coefficients for the polynomial model (Degree {best_degree}):")
for feature_name, coef in zip(feature_names, coefficients):
    print(f"{feature_name}: {coef}")

print(f"\nIntercept for the polynomial model (Degree {best_degree}): {intercept}")

# 5. Compare the performance metrics of the polynomial regression model to the baseline model.
best_model = models[best_degree]
predictions_poly = best_model.predict(X_test)

mse_poly = mean_squared_error(y_test, predictions_poly)
rmse_poly = round(np.sqrt(mse_poly), 6)

print(f"Baseline Regression Metrics: {baseline_rmse}\n")

print(f"Polynomial Regression (Degree {best_degree}) Metrics:")
print(f"Root Mean Squared Error (RMSE): {rmse_poly}")

if rmse_poly < baseline_rmse:
    improvement = ((baseline_rmse - rmse_poly) / baseline_rmse) * 100
    print(f"\nThe Polynomial Regression model (Degree {best_degree}) performed better by {improvement:.2f}% compared to the Baseline Model.")
else:
    deterioration = ((rmse_poly - baseline_rmse) / baseline_rmse) * 100
    print(f"\nThe Baseline Model performed better by {deterioration:.2f}% compared to the Polynomial Regression model (Degree {best_degree}).")

"""### Source Codes for Question 4 - Optimize the regression model for predicting total conversion"""

# Import libraries essential in Question 4
from sklearn.linear_model import Lasso, Ridge
from sklearn.model_selection import GridSearchCV

# Create a Lasso regression model
lasso = Lasso()

# Define the hyperparameters and their possible values
param_grid_lasso = {
    'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'fit_intercept': [True, False],
    'max_iter': [1000, 5000, 10000]  # This is added to ensure convergence for some datasets
}

# Grid search with cross-validation
grid_search_lasso = GridSearchCV(lasso, param_grid_lasso, cv=5, scoring='neg_mean_squared_error')
grid_search_lasso.fit(X_train, y_train)

# Get the best hyperparameters from the grid search
best_parameters_lasso = grid_search_lasso.best_params_
print(f"Best Parameters for Lasso: {best_parameters_lasso}")

# Evaluate the optimized model
lasso_best = Lasso(**best_parameters_lasso)
lasso_best.fit(X_train, y_train)
predictions_optimized_lasso = lasso_best.predict(X_test)

mse_optimized_lasso = mean_squared_error(y_test, predictions_optimized_lasso)
rmse_optimized_lasso = round(np.sqrt(mse_optimized_lasso), 6)

print(f"Optimized Lasso Regression Metrics:")
print(f"Root Mean Squared Error (RMSE): {rmse_optimized_lasso}")

# Create a Ridge regression model
ridge = Ridge()

# Define the hyperparameters and their possible values
param_grid = {
    'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'fit_intercept': [True, False],
}

# Grid search with cross-validation
grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# Get the best hyperparameters from the grid search
best_parameters = grid_search.best_params_
print(f"Best Parameters: {best_parameters} \n")

# Evaluate the optimized model
ridge_best = Ridge(**best_parameters)
ridge_best.fit(X_train, y_train)
predictions_optimized = ridge_best.predict(X_test)

mse_optimized_ridge = mean_squared_error(y_test, predictions_optimized)
rmse_optimized_ridge = round(np.sqrt(mse_optimized_ridge), 6)

print(f"Baseline Regression Metrics: {baseline_rmse}\n")
print(f"Optimized Ridge Regression Metrics:")
print(f"Root Mean Squared Error (RMSE): {rmse_optimized_ridge}")